# 异步编程的难点
JavaScript异步代码很容易失控。匿名函数闭包和在位定义使我们拥有舒适的编程体验，不需要开发者跳到代码的其它部分。这和**KISS**原则是一致的；简单地说，它确。保了代码的流程，使开发时间变短。不幸的是，牺牲了模块化、可重用、可维护性等特性，会迟早导致回调堆叠的激增，函数代码量的增加，会导致较差的代码组织。大部分情况下，并不需要创建闭包，更多的是一个原则而不是为了解决异步编程的问题。意识到我们的代码正在变得笨重，好一点的情况，预先知道它可能会变得笨重，能否相应采取最好的解决方案是区别一个新手和老手的标识。
## 创建一个简单的网络爬虫
为了解释这个问题，我们将创建一个**网络爬虫**，一个命令行应用，接受URL作为输入然后下载其内容到本地文件中。在本章展示的代码中，我们将使用一些*npm*依赖：

* *request*:一个HTTP流的调用库。
* *mkdirp*:一个循环地创建目录的小工具。

同样，我们会经常引用一个本地模块，叫做*./utilities*,包含一些应用中将会用到的帮助模块。在书中我们简单地展示必要的部分，但你可以在本书的下载包（ https://www.packtpub.com/ ）中找到完整的实现，包括包含完整依赖列表的*package.json*文件。
应用的核心功能包含在一个叫做*spider.js*的模块中。让我们看下它是怎样的。首先，加载所有要使用的依赖：

```
var request = require('request');
var fs = require('fs');
var mkdirp = require('mkdirp');
var path = require('path');
var utilities = require('./utilities');
```
然后，我们创建一个新的函数，叫做*spider()*,接收要下载的URL和一个下载完后要被触发的回调函数：

```
function spider(url, callback) {
    var filename = utilities.urlToFilename(url);
    fs.exists(filename, function (exists) {//[1]
        if (!exists) {
            console.log('Downloading ' + url)
            request(url, function (err, response, body) {//[2]
                if (err) {
                    callback(err);
                } else {
                    mkdirp(path.dirname(filename), function (err) {//[3]
                        if (err) {
                            callback(err);
                        } else {
                            fs.writeFile(filename, body, function (err) {//[4]
                                if (err) {
                                    callback(err);
                                } else {
                                    callback(null, filename, true);
                                }
                            })
                        }
                    })
                }
            })
        } else {
            callback(null, filename, false);
        }
    })
}
```
前面的函数执行了如下任务：

1. 通过检查对应文件是否已经创建来检查URL是不是已经下载了：`fs.exists(filename, function (exists) ...`
2. 如果没找到文件，使用下面的代码下载URL：`request(url, function (err, response, body)...`。
3. 然后，确保文件所在的文件夹是否存在：`mkdirp(path.dirname(filename), function (err)...`
4. 最后，把HTTP响应的主体内容写入文件：`fs.writeFile(filename, body, function (err)...`

为了完成我们的网络爬虫应用，只需触发*spider()*函数来提供一个URL作为输入（在我们的例子中，从命令行参数读取它）：

```
spider(process.argv[2], function (err, filename, downloaded) {
    if (err) {
        console.log(err);
    } else if (downloaded) {
        console.log('Completed the download of "' + filename + '"')
    } else {
        console.log('"' + filename + '"was already downloaded')
    }
});
```
现在，我们可以试用一下我们的网络爬虫了，首先要确保项目文件夹中包含*utilities.js*模块和包含完整依赖列表的*package.json*文件。然后，通过运行如下命令安装所有的依赖：`npm install`。
下一步，我们可以执行*spider*模块来下载网页的内容，通过类似于下面的命令：`node spider http://www/example.com`。

> 我们的网络爬虫需要URL中包含协议（比如，*http://*）。同样，也不要指望被重定向的HTTP链接或者类似于图片的资源会被下载，这只是一个简单的案例，来表述异步编程是怎样的。

## 回调地狱

