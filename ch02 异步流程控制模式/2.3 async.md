# async库
如果看一下已经分析的每个流程控制模式，它们可以作为构建更加通用并且易于复用的解决方案。比如，可以把无限并行执行算法封装进一个函数，它接收一组任务作为参数，并行地执行它们，当所有任务都执行完成后，触发一个给定的回调。这种封装流程控制算法到可重用函数的方式可以得到更加定义异步控制流程更加清晰的方式，这就是*async*库( https://npmjs.org/package/async ) 实现的功能。*async*库是一个流行的解决方案，通常用于Node.js和Javascript中处理异步代码。其提供了一组函数来以不同的配置执行一组任务，也提供了非常有用的帮助类来处理真对集合的异步操作。即使有一些别的库可以用来完成类似的功能，*async*因为其流行性已经成为了Node.js中的标准。

我们来直接俄使用它来展示其能力。
## 顺序执行
在实现复杂的异步流程时，`async`可以给我们极大的帮助，但给手头的问题选择哪个方法是一个难题。比如，对于这种顺序执行，有大概20种方法备选，如：`eachSeries()`,`mapSeries()`, `filterSeries()`, `rejectSeries()`, `reduce()`, `reduceRight()`,`detectSeries()`, `concatSeries()`, `series()`, `whilst()`, `doWhilst()`, `until()`,`doUntil()`, `forever()`, `waterfall()`, `compose()`, `seq()`,`applyEachSeries()`,`iterator()`, 和 `timesSeries()`。
选择正确的函数是写出整齐、可读性高的代码的重要一步，但这也需要经验和练习。在我们的例子中，我们将讲述这类函数的中的几个，但这对于理解并高效运用其它的库打下坚实的基础。
现在，为实际展示`async`的工作模式，我们会将其应用到我们的网络爬虫应用。直接基于版本2开始，第2版是采用递归的方式按顺序下载所有的链接。
首先确保我们已经在当前项目中安装了`async`库：`npm install async`。然后，我们需要从`spider.js`加载新的依赖：`var async = require('async');`。
### 顺序执行已知数量的任务
先修改`download()`。如前所述，它按顺序执行三个任务：

1. 下载URL的内容。
2. 如果文件夹不存在，就创建一个。
3. 把URL的内容保存到一个文件中。

适用于这个流程的理想方法是`async.series()`，函数签名如下：

```
async.series(tasks, [callback])
```
它接收一组`tasks`和一个回调函数，回调函数在所有任务都执行完成后触发。每个任务都是一个函数，接收一个回调函数，当每个任务执行完成后，必须执行这个回调函数：

```
function task(callback) {}
```
`async`使用和Node.js一样的回调约定，这一点非常好，此外它还能自动进行错误的传递。因此，任何一个任务执行回调时传入了错误，`async`会跳过剩下的任务，直达最终的回调。
基于这个思路，我们看如何使用`async`改造`download()`:

```
function download(url, filename, callback) {
    console.log('Downloading ' + url);
    var body;
    async.series([
        function (callback) { //[1]
            request(url, function (err, response, resBody) {
                if (err) {
                    return callback(err);
                }
                body = resBody;
                callback();
            });
        },
        mkdirp.bind(null, path.dirname(filename)), //[2]
        function (callback) { //[3]
            fs.writeFile(filename, body, callback);
        }
    ], function (err) { //[4]
        console.log('Downloaded and saved: ' + url);
        if (err) {
            return callback(err);
        }
        callback(null, body);
    });
}
```
如果你记得这个爬虫功能的回调地狱版本，我们会非常感谢`async`带给我们的任务代码组织方式。不再需要嵌套回调了，我们只需要提供一个平顺的任务列表，通常是一个异步操作一个任务，`async`会按顺序执行它们。下面是关于每个任务定义的解释：

1. 第一个任务触发了URL的下载。同时，把响应的主体存入变量`body`，使其可以在多个任务间共享。
2. 在第二个任务中，会创建存放页面下载结果的文件夹。通过运行`mkdirp()`的偏函数来实现，把要创建的文件夹的路径绑定上去。这样，可以保存一小段代码并可以增加其可读性。
3. 最后，把从URL下载的内容写入到文件中。这种情况下，就不能运行一个偏函数了，因为变量`body`只有当第一个任务完成后才能访问到。但是，依旧可以通过发掘*async*的自动错误管理机制来减少代码量，只需把函数的回调直接传入到`fs.writeFile()`函数中。
4. 当所有的任务都完成了，`async.seriers()`的回调会被触发。如此一来，只需做一些错误管理工作，然后通过`download()`的回调函数将`body`变量返回。

对于这种情况，`async.series()`的备选方案一直是`async.waterfall()`，除了可以按顺序执行任务之外，还可以把前一个的输出作为下一个的输入。对于我们的需求，可以用这种方式来传递`body`变量，直到序列的最后。作为一个练习，你可以试着试用waterfall流程来实现相同的方法，看看有什么不同。
### 顺序遍历
在前面我们已经看到，怎样按顺序执行一组已知的任务；我们使用`async.series()`来实现。也可以用它来实现网络爬虫版本2的`spiderLinks()`函数，然而，`async`为这种场景提供了更合适的帮助方法，使我们可以遍历一个集合；这个方法是`async.eachSeries()`。让我们用它来重新实现`spiderLinks()`函数（版本2，按顺序下载）如下：

```
function spiderLinks(currentUrl, body, nesting, callback) {
    if (nesting === 0) {
        return process.nextTick(callback);
    }

    var links = utilities.getPageLinks(currentUrl, body);
    if (links.length === 0) {
        return process.nextTick(callback);
    }

    async.eachSeries(links, function (link, callback) {
        spider(link, nesting - 1, callback);
    }, callback);
}
```
比较前面代码的异同，当使用`async`和纯JavaScript模式来实现一个同样的功能，我们会发现`async`在代码组织和可读性方面带给我们极大便利。
## 并行执行
`async`库不缺少处理并行流程的函数，这类函数有 `map()`、`each()`、`filter()`、`reject()`、`detect()`、`some()`、`every()`、`concat()`、`parallel()`、`applyEach()`、`times()`。它们的定义和前面在顺序执行中看到的一样，区别是提供的任务是并行的。
为了讲述其使用方法，可以试着来应用某个函数到网络爬虫的版本3上，它以无限并行的方式执行下载。
If we remember the code we used earlier to implement the sequential version of the spiderLinks() function, adapting it to make it work in parallel is a trivial task:
如果记得前面用来实现串行版本的`spiderLinks()`函数的代码，修改它使其一并行的方式运行是非常简单的：

```
function spiderLinks(currentUrl, body, nesting, callback) {
  [...]
  async.each(links, function(link, callback) {
    spider(link, nesting - 1, callback);
  }, callback);
}
```
函数其实和用于串行下载的函数一样的，只是这次用了`async.each()`而不是`async.eachSerier()`。这个例子清楚地展示了使用诸如`async`这种库给异步流程控制带来的抽象能力。代码不再绑定某个特定的执行流程了；无需专门为此写代码，大部分只是应用逻辑。
## 有限的并行执行
如果你正在想，`async`是否也可以用于限制并行任务的并发量呢？答案是肯定的，它可以！有一组方法可以用来做这件事情，即 `eachLimit()`、 `mapLimit()`、 `parallelLimit`、 `queue()`、 `cargo()` 。
接下来试着探索它们中的一个来实现网络爬虫的版本4，这个版本的爬虫以并行的方式下载链接，并且对并发数做出了限制。所幸，`async`有`async.queue()`，它的运行方式和前面章节创建的`TaskQueue`类似。`async.queue()`函数创建一个新队列，使用`worker`函数来实现一组任务，同时可以指定并发数：

```
var q = async.queue(worker, concurrency);
```
`worker()`函数接收一个要执行的任务和一个任务完成后要触发的回调函数作为参数：

```
function worker(task, callback)
```
你应该已经发现，这里的`task`可以是任何东西，并不只是一个函数。事实上，`worker`来负责用最合适的方式来处理任务。新的任务可以通过`q.push(task, callback)`来添加到队列中。当任务被处理完成后，`worker`需要触发和这个任务关联的回调。
现在，再次修改代码来实现一个并行的全局并发限制流程，使用`async.queue()`。首先，你需要创建一个新的队列：

```
var downloadQueue = async.queue(function(taskData, callback) {
  spider(taskData.link, taskData.nesting - 1, callback);
}, 2); 
```
代码非常简单。我们正在创建一个并发数为2的队列，它拥有一个`worker`,使用任务相关的数据来触发`spider()`函数。然后，实现这个`spiderLinks()`函数：

```
function spiderLinks(currentUrl, body, nesting, callback) {
    if (nesting === 0) {
        return process.nextTick(callback);
    }
    var links = utilities.getPageLinks(currentUrl, body);
    if (links.length === 0) {
        return process.nextTick(callback);
    }
    var completed = 0, errored = false;
    links.forEach(function (link) {
        var taskData = {link: link, nesting: nesting};
        downloadQueue.push(taskData, function (err) {
            if (err) {
                errored = true;
                return callback(err);
            }
            if (++completed === links.length && !errored) {
                callback();
            }
        });
    });
}
```
前面的代码会看起来非常熟悉，几乎和使用`TaskQueue`对象来实现的相同的流程。同时，在这个例子中，需要分析的是把一个新任务加入到队列中的部分。在这里，我们确保传入了一个回调函数，是我们检查是否当前页所有的下载都已经完成了，然后触发最终的回调。
多亏了`async.queue()`，你可以容易地复制`TaskQueue`对象的功能，同时可以发现，使用`async`，你可以不用从头写异步流程控制模式的代码，减少工作量，少些几行代码。


