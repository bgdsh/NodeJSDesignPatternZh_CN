# 复制与负载均衡
当分配给一台机器的资源无法再升级或者升级带来的成本上升高于简单地开启另一台服务器时，传统的多线程服务器就需要拓展了。通过使用多线程，传统的服务器能够充分利用服务器的所有处理能力，使用所有的处理器和系统。然而，一个单独的Node.js线程无法实现这个功能，因为它是单线程的并且有一个1GB的限制（在64位的机器上，可以增加到1.7GB )。这意味着Node.js应用相对于传统服务器需要更经常地拓展，即便是在单机情况下，也需要能够充分利用所有的资源。

> 在Node.js中，**竖向拓展**（为单台机器增加更多的资源）和**横向拓展**（为基础设施增加更多的机器）基本是等同的概念；都是引入相同的技术来充分利用所有的处理能力。

不要傻傻地认为这是一个劣势。相反，几近于强制的拓展在某些方面是对程序有利的，尤其是在可用性和容错方面。事实上，通过复制的方式来拓展Node.js应用是相对来说比较简单的，并且即便是没必要使用更多资源的时候，只是为了拥有冗余和容错能力。
这也迫使开发者从应用早期就考虑拓展性，确保应用不依赖任何不能跨进程、机器的资源。事实上，预先考虑的可拓展性是指每个实例都不存储不能共享的通用信息，通常是指硬件，诸如内存和磁盘。比如，在web服务器上，把会话存储在磁盘上不利于拓展；相反，使用一个共享的数据库能够确保每个实例都能访问相同的绘画信息，不管它部署在哪。
现在来介绍最基础的拓展Node.js的机制：*cluster*模块。

## 集群模块
在Node.js中，在单台机器上的不同实例间分配负载的最简单模式是使用*cluster*模块，它是核心库的一部分。*cluster*模块简化了建立同一应用的一个新实例的工作，在它们中间自动地分配链接，如下图所示：

![](../images/cluster-module.png)

**主进程**负责管理一系列的进程（**workers**），每个都代表你想拓展的实例。每个进入的链接然后被分配到被复制的worker,在他们中分配他们。

### 关于集群模块的行为备注
在Node.js的0.8和0.10中，*cluster*模块使所有的worker都共享相同的服务器链接，把在多个worker间**负载均衡**的工作留给操作系统。然而，这种方式有个问题；事实上，操作系统在worker之间分配负载的算法不是用来做网络请求的负载均衡的，而是安排线程的执行。这样一来，实例之间的分配就不均衡了；经常是一部分worker接收了大部分的负载。这种行为对于操作系统调度器是有意义的，因为它专注于把不同进程间的上下文切换减到最低。这是*cluster*模块在Node.js低于0.10版时无法发挥最大可能性的原因。
然而，从0.11.2开始情况变化了，简明的**单循环**负载均衡算法被加入到主线程中，确保了请求均衡地分配到所有的worker上。新的负载均衡算法在除了Windows之外的所有平台默认开启，可以通过全局设置*cluster.schedulingPolicy*变量来修改，使用常量*cluster.SCHED_RR*（单循环算法）或*cluster.SCHED_NONE*（操作系统处理）。

>单循环算法基于轮转平等地分发负载。第一个请求指向第一个服务，第二个指向列表的下一个，以此类推。当到达列表末尾时，循环又重新开始。这是最简单最常用的负载均衡算法之一；然而，这不是唯一的。更高级的算法允许置顶优先级，选择负载最小或者响应最快的服务。

> 你可以在下面的两个issue中找到*cluster*模块演进的细节：
> 
> * https://github.com/joyent/node/issues/3241
> * https://github.com/joyent/node/issues/3241

### 构建一个简单的HTTP服务器
现在开始创建一个实例，构建一个小的HTTP服务器，使用*cluster*模块进行复制和负载均衡。首先，需要应用可拓展；对这个例子来说，不需要太多，只是一个基本的HTTP服务器。
然我们创建一个叫做*app.js*的文件，包含如下代码：

```
var http = require('http');
var pid = process.pid;

http.createServer(function(req, res) {
  for(var i = 1e7; i > 0; i--) {}
  console.log('Handling request from ' + pid);
  res.end('Hello from ' + pid + '\n');
}).listen(8080, function() {
  console.log('Started ' + pid);
}); 
```
我们构建的这个HTTP服务对于任何请求都返回一条包含PID的消息；这样对于确定应用的那个实例处理的这个请求。同样的，来模拟真实的CPU事务，我们运行一个空的循环1000万次；没有这个，在小规模的测试时，将什么都感觉不到。

> 我们想要拓展的*app*模块可以是任何东西，也可以是使用web框架实现的，比如*express*。

我们现在检查是不是所有的实例都按预期运行，可以想平常一样运行应用，使用浏览器或者*curl*发送请求。
我们也可以来测量使用一个线程每秒能处理的请求数，我们可以使用网络测试工具来达到这个目的，比如**[siege](http://www.joedog.org/siege-home)**或Apache [ab](http://httpd.apache.org/docs/2.4/programs/ab.html)

```
siege -c 200 -t 10S http://localhost:8080”
```
使用*ab*，命令行如下：

```
ab -c 200 -t 10 http://localhost:8080/
```
前面的命令将在10秒之内请求200次。作为参考，四个处理器的系统每秒大约能处理90个事务，每个CPU的使用率只有20%。

> 请记住，我们在这一章运行的负载测试是有意的简单、最小化，用于参考和学习目的。其结果并不能为我们分析的各种技术提供一个100%准确的评估。


### 使用cluster模块进行拓展
现在我们试着使用*cluster*模块拓展我们的应用，让我们创建一个新的模块，叫做*clusterApp.js*:

```
var cluster = require('cluster');
var os = require('os');

if(cluster.isMaster) {
  var cpus = os.cpus().length;
  //start as many children as the number of CPUs
  for (var i = 0; i < cpus; i++) {      //[1]
    cluster.fork();
  }
} else {
  require('./app');           //[2]
}
```
由此可见，使用*cluster*需要很少的工作。让我们来分析一下发生了什么：

1. 当我们在命令行启动*clusteredApp*时，我们执行的是主线程。*cluster.isMaster*变量设置为*true*，我们要做的只有使用*cluster.fork()*复制当前的进程。在前面的例子中，我们开启了和CPU数量一样多的进程，来利用所有的计算能力。
2. 当*cluster.fork()*在主进程被执行，当前的主模块（*clusteredApp*）又运行了一遍，这次是worker模式（*cluster.isWorker*被设置为false）。当程序作为worker运行时，可以执行一些特定的工作。在我们的例子中，我们加载了*app*模块，启动了一个新的HTTP服务。

> 记住每个worker是一个不同的Node.js进程，有自己的事件循环、内存空间、加载的模块。

需要清楚，*cluster*模块的使用是基于递归模式的，非常容易运行一个应用的多个实例：

```
if(cluster.isMaster) {
	// fork()
} else {
	// do work
}
```
> 在罩子下，*cluster*模块使用*child_process.fork()*API（我们已经在第六章 模式菜谱 中见过），同时，主从之间也有一个沟通的渠道。worker的实例可以通过*cluster.workers*变量访问，所以给所有的进程发布一条信息将会非常简单，使用如下代码：
> 
> ```
> Object.keys(cluster.workers).forEach(function(id) {
  cluster.workers[id].send('Hello from the master');
}); 
> ```

现在，我们尝试以集群模式运行我们的HTTP服务器。我们可以像往常一样启动*clusteredApp*模块：

```
node clusteredApp
```
如果我们的及其拥有多于一个的处理器，我们可以看到一些worker被主进程启动起来了。比如，在一个拥有4个处理器的系统中，终端看起来应该是这样的：

```
Started 14107
Started 14108
Started 14109
Started 14110
```
如果我们再试着通过URL `http://localhost:8080`访问我们的服务器，可以发现每个请求都返回一个包含不同PID的信息，这意味着这些请求已经被不同的工作进程处理了，确定了负载是在他们之间分布的。
现在，我们可以再次试着加载我们的服务器：

```
siege -c 200 -t 10S http://localhost:8080
```
这样，我们可以发现在不同的处理器中间拓展应用使其性能增加了。作为参考，在拥有4个处理器的Linux系统，使用Node.js 0.10 ，性能大概增加了3倍（270 处理/秒 比 90处理/秒），CPU平均负载90%。

### 集群模式的弹性和可用性
前面提到过，拓展应用程序还会带来其它便利，尤其是维护特定服务级别的能力，即便存在故障和崩溃。这个特性也称作**弹性**，对系统的**可用性**也有很大的好处。
通过启动一个应用的多个实例，我们创建了一个冗余系统，这意味着不管一个实例因为什么原因宕机了，我们还有其它实例来响应请求。这种模式是用*cluster*模块非常容易实现。让我们来看一下它是怎样运作的！
让我们从上节的代码开始，修改*app.js*模块，让它在一个随机的事件之后崩溃：

```
// [...]
// At the end of app.js
setTimeout(function() {
  throw new Error('Ooops');
}, Math.ceil(Math.random() * 3) * 1000); 
```
这样修改之后，服务器在1到3秒之内某个随机事件会发生错误退出。在实际的环境中，这会导致我们的应用程序停止工作（响应请求），除非我们使用一些外部工具来监控它的状态，自动重启它。然而，如果我们只有一个实例，将会有一个不可忽略的延迟。这意为这，在这期间，应用程序是不可用的。拥有多个实例将会确保我们一直有一个备份系统来响应请求，即便一个工作进程崩溃了。
使用*cluster*模块，我们要做的是一旦检测到某一个进程因为错误终止了，就启动一个新的工作进程。我们接下来修改*clusteredApp.js*模块来把这个加进去：

```
if(cluster.isMaster) {
  // [...]
  cluster.on('exit', function(worker, code) {
    if(code != 0 && !worker.suicide) {
      console.log('Worker crashed. Starting a new worker');
      cluster.fork();
    }
  });
} else {
  require('./app');
}
```
在前面的代码中，一旦主进程接收到了*exit*事件，我们检查进程是有意退出的还是因错误退出的；我们通过检查状态的*code*和*worker.suicide*变量，这个变量是指工作进程是否是被主进程手动结束的。如果确定进程是因为错误结束的，启动一个新的工作进程。有趣的是，我们发现当崩溃的工作进程重启时，其它的工作进程依然可以响应请求，因此不会影响应用的可用性。
为了测试这个假设，可以尝试对我们的服务使用*siege*再次进行压力测试。当压力测试完成时，我们可以发现*siege*还有一个指示应用可用性的指标。预期的结果如下所示：

```
Transactions:           3027 hits
Availability:           99.31 %
[...]
Failed transactions:         21 
```
请记住，这个结果可能变化很多；它严重依赖正在运行的实例数量和测试时崩溃的次数，但应该给出一个能够指示我们解决方案工作状态的指示器。前面的数字告诉我们不管我们的应用持续崩溃，3027次请求只失败了21次。在我们构建的测试场景中，大部分失败请求被已经建立的连接的中断引起。
事实上，当这种情况发生了，*siege*将会打印如下错误：

```
[error] socket: read error Connection reset by peer sock.c:479: Connection reset by peer
```
不幸的是，针对这种失败，我们可做的事情并不多，特别是当应用程序因为崩溃退出时。然而，我们的解决方案注定可用并且对于经常崩溃的应用，它的可用性一点也不坏。
### 零停机时间重启
在Node.js应用中，在更新代码时程序需要重启。在这种情况下，拥有多个实例可以维持应用的可用性。
当我们计划重启应用并更新代码，在程序重启到可以提供服务之间有一个空窗期。如果更新的是我们的个人博客，这是可以接受的，但是对于一个专业的应用程序来说这不能算是一个可行方案。专业程序有一个SLA(服务水准协议)或者是作为**持续交付**一部分频繁更新。解决方案是实现一个**零停机时间重启**，当更新代码的时候不会影响服务的可用性。
使用*cluster*模块，这又是一项非常简单的工作，这个模式指每次重启一个工作线程。这样，其余的工作线程可以继续运转，维护服务的可用。
我们把这个新特性加入集群化的服务器。我们需要做的就是增加一些新的代码，它们会在主线程执行（*clusteredApp.js*文件）：

```
if(cluster.isMaster) {
  // [...]
  
  process.on('SIGUSR2', function() {         //[1]
    console.log('Restarting workers');
    var workers = Object.keys(cluster.workers);

    function restartWorker(i) {         //[2]
      if(i >= workers.length) return;
      var worker = cluster.workers[workers[i]];
      console.log('Stopping worker: ' + worker.process.pid);
      worker.disconnect();           //[3]

      worker.on('exit', function() {
        if(!worker.suicide) return;
        var newWorker = cluster.fork();      //[4]
        newWorker.on('listening', function() {
          restartWorker(i + 1);         //[5]
        });
      });
    }
    restartWorker(0);
  });
} else {
  require('./app');
}
```
下面是前面代码块的运作原理：

1. 当接收到*SIGUSR2*的信号时，触发工作进程的重启。
2. 定义一个迭代器函数叫做*restartWorker()*。实现了一个异步顺序迭代模式，遍历*cluster.workers*对象。
3. *restartWorker()*的第一个任务是通过*worker.disconnect()*优雅地结束一个工作进程。
4. 当被停止的进程退出时，我们可以创建一个新的工作进程。
5. 当一个新的工作进程已经就位，并且监听新的连接，我们可以继续通过触发迭代的另一步来重启另一个工作进程。

> 因为我们的程序使用UNIX信号，它不会在Windows系统上正常地运行。信号是最简单的机制来实时我们的方案。但这并不是唯一的方式；事实上，其它方式包括监听从连接来的命令，管道，或者标准的输入。

现在我们可以通过运行*clusteredApp*,然后发送*SIGUSR2*信号，测试我们的0停机时间重启。首先，我们要获得主进程的PID；下面的命令用于从所有的运行中的进程中把它识别出来：

```
ps af
```
主进程是一系列*node*进程的父进程。一旦我们有了要找的PID，可以给它发送一个信号：
```
kill -SIGUSR2 <PID>
```
这时，*clusteredApp*应用的输出应该显示如下：

```
Restarting workers
Stopping worker: 19389
Started 19407
Stopping worker: 19390
Started 19409
```
我们可以再次使用*siege*来验证重启工作进程没有对程序的可用性造成影响。

> pm2(https://github.com/Unitech/pm2)是一个小工具，基于*cluster*，提供负载均衡、进程监控、零停机时间重启以及其它很好的功能。

## 处理状态沟通
*cluster*模块不能很好地和状态沟通一起工作，因为被应用程序维护的状态不会在多个实例间共享。这是因为属于同一会话的不同请求可能会被不同的应用实例来处理。这个问题不仅限于*cluster*模块，它存在于任何种类的无状态的负载均衡算法。考虑下图的例子：
[](../images/stateful_communication.png)
用户*john*开始时发送了一个请求，验证自己的身份，但操作的结果注册到了本地（比如，在内存中），所以只有接收到验证请求的实例（*Instance A*）知道John被成功地认证。当John发送一个新的请求时，负载均衡可能把它指向一个不同的实例，它可能没有*John*的验证细节，因此拒绝了操作。我们刚描述的应用没法像这样拓展，但幸运的是，有两种简单的方式可以用来解决这个问题。
### 多实例共享状态

### 棘手的负载均衡

## 通过反向代理拓展

### 使用Nginx实现负载均衡

## 使用服务注册

### 使用http-proxy和seaport实现动态负载均衡

## 点到点的负载均衡

### 实现一个可以平衡多台服务器请求的HTTP客户端



