# 使用流进行异步流程控制
看过这么多的例子，大家应该已经非常清楚了流不光可以用于处理I/O，而且是一种用于处理任何种类数据的编程模式。但是其优势不光体现在表面；流也可以用于把异步控制流转变为流程控制，本节将讨论这些内容。
## 顺序执行
默认情况下，流会按顺序处理数据，比如，在前一次调用完成并执行`callback()`之前，`Transform`的`_transform()`方法不会执行来处理下一块数据。这是流的一个重要特性，是以正确的顺序处理每一块数据的关键，但这也可以被发掘用于把流转换为一种优雅的流程控制解决方案。

代码比过多的解释更能说明问题，那么接下来就展示一个例子，来展示如何使用流来按顺序执行异步任务。现在创建一个函数，连接输入的一组文件，并确保其顺序和给定的顺序一致。创建一个新的模块`concatFiles.js`并从其依赖开始定义其内容：

```
var fromArray = require('from2-array');
var through = require('through2');
var fs = require('fs');
```
我们将使用`through2`来简化`Transform`流的创建，使用`from2-array`从一组对象创建一个可读流。

接下来，我们可以定义`concatFiles()`函数：

```
function concatFiles(destination, files, callback) {
  var destStream = fs.createWriteStream(destination);
  
  fromArray.obj(files)             //[1]
    .pipe(through.obj(function(file, enc, done) {   //[2]
      var src = fs.createReadStream(file);
      src.pipe(destStream, {end: false});
      src.on('end', function() {         //[3]
        done();
      });
    }))
    .on('finish', function() {         //[4]
      destStream.end();
      callback();
    });
}
module.exports = concatFiles;
```
前面的函数实现了对`files`数组按顺序遍历，并将其转换到一个流中。下面是对这个过程的解释：

1. 首先，使用`from2-array`来从`files`数组创建一个`Readable`流。
2. 接下来，我们创建一个*through*（`Transform`）流，来处理队列中的每个文件。对于每个文件，创建一个`Readable`流，然后将其导入到`destStream`，它代表输出到的文件。我们需要确保读取完成后不关闭`destStream`，通过指定`pipe()`的`{end:false}`选项。
3. 当源文件的所有内容都被导入到`destStream`，我们触发`done()`，触发对下一个文件的处理。
4. 当所有的`files`都被处理之后，会触发`finish`事件；最后，就可以结束`destStream`并触发`concatFiles()`的`callback()`函数，它表示整个操作的结束。

我们现在可以尝试使用刚创建的这个小模块了。创建一个小模块叫做`concat.js`：

```
var concatFiles = require('./concatFiles');
concatFiles(process.argv[2], process.argv.slice(3), function() {
  console.log('Files concatenated succesfully');
});
```
通过传入第一个命令参数作为目标文件地址，后面的几个参数表示要合并的文件，比如：

```
node concat allTogether.txt file1.txt file2.txt 
```
这会创建一个叫做`allTogether.txt`的文件，并且包含`file1.txt`和`file2.txt`的内容。
通过`concatFiles()`函数，只用流就可以完成异步顺序遍历。如在第二章*异步流程控制模式*见到的，如果用纯JavaScript实现的话，需要用到遍历器；或者使用诸如`async`这种外部库。现在我要给提供给大家一种新的方式来达到这一目标，而且看起来更加优雅紧凑。

> 模式：使用流或者流的组合，很容易达成使一组任务顺序执行的目标。
## 无序的并行执行
我们刚看到，流按顺序处理每一个数据块，但有时这也会带来瓶颈，因为这样一来我们就没法实现并发。如果我们必须对每一块执行一个慢速的异步操作，并发执行可以加速整个过程。当然了，这个操作只能在多个数据块之间没有关系时使用，这在对象流的应用中非常常见，但是在字节流的应用中并不常见。

> 注意：字节流不可以用在顺序对与数据处理非常重要的情形。

为了并行地执行`Transform`流，我们可以应用在第二章*异步控制流程*中学到的模式，但需要一些适配工作，使其作用于流。接下来我们看其工作模式。
### 实现无序并行流
现在我们立刻展示一个实例；创建一个模块叫做`parallelStream.js`并定义一个通用的`Transform`流来并行地执行给定的变换函数。我们现在定义其构造函数：

```
var stream = require('stream');
var util = require('util');

function ParallelStream(userTransform) {
  stream.Transform.call(this, {objectMode: true});
  this.userTransform = userTransform;
  this.running = 0;
  this.terminateCallback = null;
}
util.inherits(ParallelStream, stream.Transform);
```
这个构造函数接收一个`userTransform()`函数，把它保存为一个实例变量；我们同时触发其父类的构造函数，为了方便，默认开启对象模式。
接下来实现`_transform()`方法：

```
ParallelStream.prototype._transform =
  function(chunk, enc, done) {
    this.running++;
    this.userTransform(chunk, enc, this._onComplete.bind(this));
    done();
  }
```
在`_transform()`方法中，我们执行了`userTransform()`函数，然后增加正在运行中的任务数；最后，通过调用`done()`来通知当前的转换步骤已经完成。触发另一个对象处理的技巧正在于此；我们无需等待`userTransform()`函数执行完之后才调用`done()`，相反，我们立刻触发`done()`。另外，我们给`userTransform()`提供了一个特殊的回调，即`this._onComplete()`方法（等会我们就定义这个方法），它使我们当`userTransform()`完成时就收到通知。
接下来，实现`_flush()`方法：

```
ParallelStream.prototype._flush = function(done) {
  if(this.running > 0) {
    this.terminateCallback = done;
  } else {
    done();
  }
}
```
`_flush()`方法在流结束的前一刻触发，所以，如果还有任务在运行中，我们可以通过不触发`done()`回调来阻断`finish`事件的触发；相反，我们将其赋值给`this.terminateCallback()`变量。在`_onComplete()`方法的实现中，我们可以看到流是怎样正常终止的：

```
ParallelStream.prototype._onComplete = function(err) {
  this.running--;
  if(err) {
    return this.emit('error', err);
  }
  if(this.running === 0) {
    this.terminateCallback && this.terminateCallback();
  }
}
module.exports = ParallelStream; 
```
每当一个异步任务结束时，`_onComplete()`方法就会被触发。他会检查是否还有运行中的人物，如果没有了，就触发`this.terminateCallback()`函数，这将会使流结束，释放在`_flush()`方法中劫持的`finish`事件。
我们刚构建的`ParallelStream`类使我们可以非常容易地创建一个`Transform`流，它可以并发地执行任务，但也有一定的缺陷：它不会保存各个对象的接收顺序。事实上，异步操作可以随时完成并推送数据，和它们是在何时启动的无关。这样我们就可以发现，这种方式不适用于字节流，因其顺序非常重要，但对于对象类型的字节流是非常有用的。
### 实现一个URL状态监控程序
接下来，我们用`ParallelStream`来举一个具体的例子。我们假设我们想构建一个简单的服务来监控一个很大的URL列表的状态。我们假设这些所有的URL都存在于一个单独的文件中并且使用换行符分割。
流可以针对这种问题提供一种非常有效并且优雅的解决方案，尤其是使用`ParallelStreams`来并行地检查URL。
我们构建一个简单的应用，模块名叫做`checkUrls.js`：

```
var fs = require('fs');
var split = require('split');
var request = require('request');
var ParallelStream = require('./parallelStream');
fs.createReadStream(process.argv[2])    //[1]
  .pipe(split())           //[2]
  .pipe(new ParallelStream(function(url, enc, done) {     //[3]
    if(!url) return done();
    var self = this;
    request.head(url, function(err, response) {
      self.push(url + ' is ' + (err ? 'down' : 'up') + '\n');
      done();
    });
  }))
  .pipe(fs.createWriteStream('results.txt'))       //[4]
  .on('finish', function() {
    console.log('All urls were checked');
  });
```
如上面的代码所示，使用流之后，我们的代码看起来非常优雅简洁；让我们看一下其工作原理：
1. 首先，我们基于输入的文件创建`Readable`流。
2. 我们把输入文件的内容通过`split`( https://www.npmjs.com/package/split )导入内容，`Transform`流确保把每一行输出为一个代码块。
3. 然后，使用`ParallelStream`来检查URL。我们通过发送一个`HEAD`请求并等待相应来实现。当回调被触发，我们把操作的结果传递到下游。
4. 最后，所有的结果都被导入到一个文件`results.txt`。

现在，我们可以运行`checkUrls`模块，使用如下命令：

```
node checkUrls urlList.txt
```
`urlList.txt`包含URL的列表，比如：

```
http://www.example.com
http://www.example.com/link1
http://thiswillbedownforsure.com
```
当命令结束运行后，我们可以看到一个新创建的`results.txt`。这个文件包含操作的结果，比如：

```
http://thiswillbedownforsure.com is down
http://www.example.com/link1 is up
http://www.example.com is up
```
很有可能结果写入的顺序和输入文件中指定的URL的顺序并不相同。这是我们的流是并行执行的，而无需指定流中每一块的顺序。
> 出于好奇，你可能想把`ParallelStream`替换为`through2`流，比较他们俩的行为和性能（你可以在练习中进行这个操作）。我们会发现，使用`through2`会更慢一些，因为每个URL是按顺序检查的，但存在`results.txt`中的结果也会保持这个顺序。

## 有限的无序并行执行
如果要对包含成千上万个URL的文件运行`checkUrls`程序，我们将会遇到麻烦。我们的应用将会同时创建数量不可控的连接，同时发送大量数据，可能会破坏整个系统中程序的可用性和稳定性。我们都知道，控制负载和资源的使用的方法是限制并行任务的数量。
修改前一章的`parallelStream.js`为`limitedParallelStreams.js`模块：

```
function LimitedParallelStream(concurrency, userTransform) {
  stream.Transform.call(this, {objectMode: true});
  this.userTransform = userTransform;
  this.running = 0;
  this.terminateCallback = null;
  this.continueCallback = null;
  this.concurrency = concurrency;
}
```
我们需要接收一个`concurrency`限制作为参数，这次我们需要保存两个回调函数，一个是用于处理任何挂起的`_transform`方法（`continueCallback`），另一个是`flush`方法的回调（`terminateCallback`）。
接下来是修改`_transform()`方法：

```
LimitedParallelStream.prototype._transform =
function(chunk, enc, done) {
    this.running++;
    this.userTransform(chunk, enc, this._onComplete.bind(this));
    if(this.running < this.concurrency) {
      done();
    } else {
      this.continueCallback = done;
    }
  }
```
这次在`_transform()`方法中，在执行`done()`之前要确定是否还有执行任务的名额，并触发下一个任务的执行。如果已经达到了处于运行状态的流的数量的最大值，只需将`done()`回调保存到`continueCallback`变量，在某个任务完成时就会被触发。
`_flush`方法和在`ParallelStream`类中一样，所以接下来直接修改`_onComplete()`方法：

```
LimitedParallelStream.prototype._onComplete =
  function(err, chunk) {
    this.running--;
    if(err) {
      return this.emit('error', err);
    }
    var tmpCallback = this.continueCallback;
    this.continueCallback = null;
    tmpCallback && tmpCallback();
    if(this.running === 0) {
      this.terminateCallback && this.terminateCallback();
    }
  }
```
每次一个任务完成，我们会触发任何保存的`continueCallback()`，这会使流变成非阻塞状态，触发下一项执行。
这就是`limitedParallelStream`模块的内容；我们现在就可以在`checkUrls`模块中，取代原来的`ParallelStream`并且把并发数限制在我们设置的数值上。

### 有序的并行执行
我们前面创建的并行流将会打乱数据产出的顺序，但有时候应该避免这种情况发生；有时确实需要每一块都按照接收到的顺序产出。实现这种情况并非毫无可能，我们依旧可以并行地运行转换函数；我们只需对每个任务产出的数据进行排序，那么就可以保证其顺序和接收到的顺序一致。
这个技术需要用到缓冲来记录每个运行的任务产出的数据块。简便起见，我们不会提供这种流的实现，因为对于这本书来说有点啰嗦了；我们将要使用实现了这个功能的包，比如`through2-parallel`（ https://www.npmjs.com/package/through2-parallel ）。
我们可以通过修改`checkUrls`模块来检查有序并行执行的情况。假设我们想要结果按照输入文件中的顺序写入，同时检查并发地任务数。我们可以使用`through2-parallel`来实现这个功能：

```
[...]
var throughParallel = require('through2-parallel');

fs.createReadStream(process.argv[2])
  .pipe(split())
  .pipe(throughParallel.obj({concurrency: 2},
    function(url, enc, done) {
      [...]
    })
  )
  .pipe(fs.createWriteStream('results.txt'))
  .on('finish', function() {
    console.log('All urls were checked');
  });

```
如我们所见，`through2-parallel`的接口和`through2`的类似；唯一的区别是可以为我们提供的转换函数指定并发数的限制。如果要尝试运行这个新版本的`checkUrls`，我们会看到列入`results.txt`中的结果和输入文件中的URL出现顺序一致。

> 有一个非常重要的点，即使输出的顺序和输入的顺序一致，异步任务还是并行执行的并且可以以任何顺序完成。

到此，我们结束了对与异步流程控制的分析；接下来，我们将着眼于一些简单的管道模式。

